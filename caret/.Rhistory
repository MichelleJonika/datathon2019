# }
# good.sim <- F
# rate <- .2
# while(good.sim == F){
#   disc.trait <- sim.char(phy = trees,
#                          par = matrix(c(-rate, 0, rate, 0), 2),
#                          model = 'discrete',
#                          root = 1)
#   if(4 < sum(disc.trait == min(disc.trait)) &&
#      sum(disc.trait == min(disc.trait)) < 26){
#     good.sim <- T
#   }
# }
# names(disc.trait) <- trees$tip.label
# anc.state.dt <- make.simmap(trees, disc.trait,
#                             model = matrix(c(0,0,1,0), 2),
#                             nsim = 1,
#                             pi = c(1,0),
#                             message = F)
load('../Data/Fig1DiscSimMap.RData')
plotSimmap(anc.state.dt, lwd = 4, ftype = 'off')
legend(x = 'bottomleft', legend = c('Ancestral','Derived'), col = c('black', 'red'), pch = 15, bty = 'n')
fig_label('B',cex = 2.5)
pies <- array(dim = c(anc.state.dt$Nnode, 3))
pies[1:4,] <- rep.row(c(1,0,0),4)
pies[5,] <- t(c(0,0,1))
pies[6:7,] <- rep.row(c(0,1,0),2)
pies[8:11,] <- rep.row(c(1,0,0),4)
pies[12,] <- t(c(0,0,1))
pies[13:20,] <- rep.row(c(1,0,0),8)
pies[21,] <- t(c(0,0,1))
pies[22:23,] <- rep.row(c(0,1,0),2)
pies[24:29,] <- rep.row(c(1,0,0),6)
# plot(trees, tip.color = 'transparent', edge.width = 3)
plotSimmap(anc.state.dt, lwd = 4, ftype = 'off')
nodelabels(pie = pies, piecol = c('blue','green', 'red'),cex = .8)
legend(x = 'bottomleft', legend = c('Ancestral','Producing (Ancestral)','Derived'),
col = c('blue', 'red','green'), pch = 16, bg="transparent", bty = 'n')
fig_label('C',cex = 2.5)
ss_nodes <- anc.state.dt$mapped.edge[, 1] > 0 &
anc.state.dt$mapped.edge[, 2] > 0
wanted_branches <- ss_nodes[ss_nodes == T]
wanted_nodes <- names(wanted_branches)
wanted_nodes <- gsub(",.*", "", wanted_nodes)
producing.nodes <- unique(wanted_nodes)
anc.states <- anc.ML(trees, cont.trait, model = "BM")
orig.val <- mean(anc.states$ace[names(anc.states$ace) %in% producing.nodes])
null.orig.val <- vector(length = 1000)
number.of.trans <- length(producing.nodes)
anc.dt <- anc.state.dt
anc.ct <- anc.states
node.states <- describe.simmap(anc.dt)$states
anc.cond.nodes <- anc.ct$ace[names(anc.ct$ace) %in% names(node.states)[node.states != '2']]
for (j in 1:1000){
# set.seed(j)
null.orig.val[j] <- mean(sample(anc.cond.nodes,
length(producing.nodes)))
}
par(mar = c(5,5,1,1) + .1)
plot(density(null.orig.val, bw = .025), ylab = 'Frequency', xlab = 'Mean Cont Trait', main = '')
abline(v = orig.val, col = 'red')
legend(x = 'topright', legend = c('Observed','Null'), col = c('red', 'black'), pch = 15, bty = 'n')
fig_label('D',cex = 2.5)
fig_label <- function(text, region="figure", pos="topleft", cex=NULL, ...) {
region <- match.arg(region, c("figure", "plot", "device"))
pos <- match.arg(pos, c("topleft", "top", "topright",
"left", "center", "right",
"bottomleft", "bottom", "bottomright"))
if(region %in% c("figure", "device")) {
ds <- dev.size("in")
# xy coordinates of device corners in user coordinates
x <- grconvertX(c(0, ds[1]), from="in", to="user")
y <- grconvertY(c(0, ds[2]), from="in", to="user")
# fragment of the device we use to plot
if(region == "figure") {
# account for the fragment of the device that
# the figure is using
fig <- par("fig")
dx <- (x[2] - x[1])
dy <- (y[2] - y[1])
x <- x[1] + dx * fig[1:2]
y <- y[1] + dy * fig[3:4]
}
}
# much simpler if in plotting region
if(region == "plot") {
u <- par("usr")
x <- u[1:2]
y <- u[3:4]
}
sw <- strwidth(text, cex=cex) * 60/100
sh <- strheight(text, cex=cex) * 60/100
x1 <- switch(pos,
topleft     =x[1] + sw,
left        =x[1] + sw,
bottomleft  =x[1] + sw,
top         =(x[1] + x[2])/2,
center      =(x[1] + x[2])/2,
bottom      =(x[1] + x[2])/2,
topright    =x[2] - sw,
right       =x[2] - sw,
bottomright =x[2] - sw)
y1 <- switch(pos,
topleft     =y[2] - sh,
top         =y[2] - sh,
topright    =y[2] - sh,
left        =(y[1] + y[2])/2,
center      =(y[1] + y[2])/2,
right       =(y[1] + y[2])/2,
bottomleft  =y[1] + sh,
bottom      =y[1] + sh,
bottomright =y[1] + sh)
old.par <- par(xpd=NA)
on.exit(par(old.par))
text(x1, y1, text, cex=cex, ...)
return(invisible(c(x,y)))
}
rep.row<-function(x,n){
matrix(rep(x,each=n),nrow=n)
}
library(plotfunctions)
library(R.utils)
library(phytools)
library(diversitree)
library(geiger)
library(doSNOW)
library(foreach)
##### Fig 1 #####
par(mfrow = c(2,2), mar = c(4,4,6,6) + .1)
# trees <- trees(pars = c(3,1),
#                        type = "bd",
#                        n = 1,
#                        max.taxa = 30,
#                        include.extinct = F)[[1]]
# trees$edge.length <- trees$edge.length / max(branching.times(trees))
# cont.trait <- sim.char(trees, 0.2, model = 'BM')
# names(cont.trait) <- trees$tip.label
load('../Data/Fig1Tree.RData')
load('../Data/Fig1ContTrait.RData')
smp <- contMap(trees,cont.trait, ftype = 'off', legend = F, lims = c(.24,2), plot = F)
n<-length(smp$cols)
## change to grey scale
smp$cols[1:n]<-rainbow(n, end = 4/6)
plot(smp, legend = F,ftype = 'off')
gradientLegend(depth = .03, valRange = c(.24,2), side = 1, pos = .17, color = rainbow(n, end = 4/6))
legend(x = 'bottomleft', legend = '', title = '              Cont Trait Value', bg="transparent", bty = 'n')
fig_label('A',cex = 2.5)
# cont.trait.AC <- anc.ML(trees, cont.trait, model = "BM")
# branch.means <- c()
# branch.names <- c()
# for(j in 1:nrow(trees$edge)){
#   node.o.int <- trees$edge[j,1]
#   if(node.o.int <= 30){
#     one <- cont.trait[node.o.int]
#   }else{
#     one <- cont.trait.AC$ace[names(cont.trait.AC$ace) == as.character(node.o.int)]
#   }
#   node.o.int <- trees$edge[j,2]
#   if(node.o.int <= 30){
#     two <- cont.trait[node.o.int]
#   }else{
#     two <- cont.trait.AC$ace[names(cont.trait.AC$ace) == as.character(node.o.int)]
#   }
#   branch.means <- c(branch.means, mean(one, two))
#   branch.names <- c(branch.names, paste(as.character(trees$edge[j,1]),as.character(trees$edge[j,2])))
# }
# names(branch.means) <- branch.names
# rm(branch.names)
# upper <- summary(branch.means)[[5]]
# lower <- summary(branch.means)[[2]]
# scale.factor <- 50
# alt.tree <- trees
# for(j in 1:length(branch.means)){
#   if(branch.means[j] < lower){alt.tree$edge.length[j] <- alt.tree$edge.length[j] / scale.factor}
#   if(branch.means[j] > upper){alt.tree$edge.length[j] <- alt.tree$edge.length[j] * scale.factor}
# }
# good.sim <- F
# rate <- .2
# while(good.sim == F){
#   disc.trait <- sim.char(phy = trees,
#                          par = matrix(c(-rate, 0, rate, 0), 2),
#                          model = 'discrete',
#                          root = 1)
#   if(4 < sum(disc.trait == min(disc.trait)) &&
#      sum(disc.trait == min(disc.trait)) < 26){
#     good.sim <- T
#   }
# }
# names(disc.trait) <- trees$tip.label
# anc.state.dt <- make.simmap(trees, disc.trait,
#                             model = matrix(c(0,0,1,0), 2),
#                             nsim = 1,
#                             pi = c(1,0),
#                             message = F)
load('../Data/Fig1DiscSimMap.RData')
plotSimmap(anc.state.dt, lwd = 4, ftype = 'off')
legend(x = 'bottomleft', legend = c('Ancestral','Derived'), col = c('black', 'red'), pch = 15, bty = 'n')
fig_label('B',cex = 2.5)
pies <- array(dim = c(anc.state.dt$Nnode, 3))
pies[1:4,] <- rep.row(c(1,0,0),4)
pies[5,] <- t(c(0,0,1))
pies[6:7,] <- rep.row(c(0,1,0),2)
pies[8:11,] <- rep.row(c(1,0,0),4)
pies[12,] <- t(c(0,0,1))
pies[13:20,] <- rep.row(c(1,0,0),8)
pies[21,] <- t(c(0,0,1))
pies[22:23,] <- rep.row(c(0,1,0),2)
pies[24:29,] <- rep.row(c(1,0,0),6)
# plot(trees, tip.color = 'transparent', edge.width = 3)
plotSimmap(anc.state.dt, lwd = 4, ftype = 'off')
nodelabels(pie = pies, piecol = c('blue','green', 'red'),cex = .8)
legend(x = 'bottomleft', legend = c('Ancestral','Producing (Ancestral)','Derived'),
col = c('blue', 'red','green'), pch = 16, bg="transparent", bty = 'n')
fig_label('C',cex = 2.5)
ss_nodes <- anc.state.dt$mapped.edge[, 1] > 0 &
anc.state.dt$mapped.edge[, 2] > 0
wanted_branches <- ss_nodes[ss_nodes == T]
wanted_nodes <- names(wanted_branches)
wanted_nodes <- gsub(",.*", "", wanted_nodes)
producing.nodes <- unique(wanted_nodes)
anc.states <- anc.ML(trees, cont.trait, model = "BM")
orig.val <- mean(anc.states$ace[names(anc.states$ace) %in% producing.nodes])
null.orig.val <- vector(length = 1000)
number.of.trans <- length(producing.nodes)
anc.dt <- anc.state.dt
anc.ct <- anc.states
node.states <- describe.simmap(anc.dt)$states
anc.cond.nodes <- anc.ct$ace[names(anc.ct$ace) %in% names(node.states)[node.states != '2']]
for (j in 1:1000){
# set.seed(j)
null.orig.val[j] <- mean(sample(anc.cond.nodes,
length(producing.nodes)))
}
par(mar = c(5,5,1,1) + .1)
plot(density(null.orig.val, bw = .025), ylab = 'Frequency', xlab = 'Mean Cont Trait', main = '')
abline(v = orig.val, col = 'red')
legend(x = 'topright', legend = c('Observed','Null'), col = c('red', 'black'), pch = 15, bty = 'n')
fig_label('D',cex = 2.5)
?rcoal
??rdcoal
??rcoal
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
election <- read.csv('CleanedElection.csv')
setwd("~/GitHub/datathon2019/caret")
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
election <- read.csv('CleanedElection.csv')
# Structure of the dataframe
str(orange)
str(election)
## visualize the importance of certain predictor variables using boxplots
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# view data
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
# Convert to dataframe
trainData <- data.frame(trainData_mat)
# See the structure of the new dataset
str(trainData)
## preprocessing our data by converting all numeric data to be scaled between 0 and 1
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)
# Append the Y variable
trainData$Purchase <- y
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
## visualize the importance of certain predictor variables using boxplots
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# In this case, For a variable to be important, I would expect the density curves to be significantly different
# for the 2 classes, both in terms of the height (kurtosis) and placement (skewness).
# Take a look at the density curves of the two categories for ‘LoyalCH’, ‘STORE’,
# ‘StoreID’, ‘WeekofPurchase’. Are they different?
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "density",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
library('e1071')
library(e1071)
install.packages(e1071)
install.packages('e1071')
# install.packages('e1071')
library(e1071)
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
lmProfile
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# view data
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
# Convert to dataframe
trainData <- data.frame(trainData_mat)
# See the structure of the new dataset
str(trainData)
## preprocessing our data by converting all numeric data to be scaled between 0 and 1
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)
# Append the Y variable
trainData$Purchase <- y
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
## visualize the importance of certain predictor variables using boxplots
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# In this case, For a variable to be important, I would expect the density curves to be significantly different
# for the 2 classes, both in terms of the height (kurtosis) and placement (skewness).
# Take a look at the density curves of the two categories for ‘LoyalCH’, ‘STORE’,
# ‘StoreID’, ‘WeekofPurchase’. Are they different?
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "density",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# install.packages('e1071')
library(e1071)
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
lmProfile
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
modelLookup('earth')
# Set the seed for reproducibility
set.seed(100)
# Train the model using randomForest and predict on the training data itself.
model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)
plot(model_mars, main="Model Accuracies with MARS")
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")
# Step 1: Impute missing values
testData2 <- predict(preProcess_missingdata_model, testData)
# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)
# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)
# View
head(testData4[, 1:10])
# Predict on testData
predicted <- predict(model_mars, testData4)
head(predicted)
# Compute the confusion matrix
confusionMatrix(reference = testData$Purchase, data = predicted, mode='everything', positive='MM')
# Define the training control
fitControl <- trainControl(
method = 'cv',                   # k-fold cross validation
number = 5,                      # number of folds
savePredictions = 'final',       # saves predictions for optimal tuning parameter
classProbs = T,                  # should class probabilities be returned
summaryFunction=twoClassSummary  # results summary function
)
# Step 1: Tune hyper parameters by setting tuneLength
set.seed(100)
model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl)
model_mars2
# Step 2: Predict on testData and Compute the confusion matrix
predicted2 <- predict(model_mars2, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted2, mode='everything', positive='MM')
# Step 1: Define the tuneGrid
marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10),
degree = c(1, 2, 3))
# Step 2: Tune hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)
model_mars3
# Step 3: Predict on testData and Compute the confusion matrix
predicted3 <- predict(model_mars3, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted3, mode='everything', positive='MM')
##  Training Adaboost
set.seed(100)
# Train the model using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
model_adaboost
##  Training Random Forest
set.seed(100)
# Train the model using rf
model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)
model_rf
##  Training xgBoost Dart
set.seed(100)
# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
model_xgbDART
##  Training SVM
set.seed(100)
# Train the model using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)
install.packages('diplr')
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
#chooseCRANmirror(graphics = FALSE, ind=1)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
