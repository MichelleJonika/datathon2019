head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# view data
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
# Convert to dataframe
trainData <- data.frame(trainData_mat)
# See the structure of the new dataset
str(trainData)
## preprocessing our data by converting all numeric data to be scaled between 0 and 1
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)
# Append the Y variable
trainData$Purchase <- y
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
## visualize the importance of certain predictor variables using boxplots
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# In this case, For a variable to be important, I would expect the density curves to be significantly different
# for the 2 classes, both in terms of the height (kurtosis) and placement (skewness).
# Take a look at the density curves of the two categories for ‘LoyalCH’, ‘STORE’,
# ‘StoreID’, ‘WeekofPurchase’. Are they different?
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "density",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# install.packages('e1071')
library(e1071)
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
##  Training Adaboost
set.seed(100)
# Train the model using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
model_adaboost
##  Training Random Forest
set.seed(100)
# Train the model using rf
model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)
model_rf
##  Training xgBoost Dart
set.seed(100)
# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
model_xgbDART
##  Training SVM
set.seed(100)
# Train the model using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# view data
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]
# Structure of the dataframe
str(orange)
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
##  creating the training data set. Leaving the last 20% for testing
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# view data
library(skimr)
skimmed <- skim_to_wide(trainData)
skimmed[, c(1:5, 9:11, 13, 15:16)]
skim_to_wide(trainData)
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
# Convert to dataframe
trainData <- data.frame(trainData_mat)
# See the structure of the new dataset
str(trainData)
## preprocessing our data by converting all numeric data to be scaled between 0 and 1
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)
# Append the Y variable
trainData$Purchase <- y
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
## visualize the importance of certain predictor variables using boxplots
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# In this case, For a variable to be important, I would expect the density curves to be significantly different
# for the 2 classes, both in terms of the height (kurtosis) and placement (skewness).
# Take a look at the density curves of the two categories for ‘LoyalCH’, ‘STORE’,
# ‘StoreID’, ‘WeekofPurchase’. Are they different?
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "density",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# install.packages('e1071')
library(e1071)
# install.packages('e1071')
library(e1071)
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
modelLookup('earth')
# Set the seed for reproducibility
set.seed(100)
# Train the model using randomForest and predict on the training data itself.
model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)
plot(model_mars, main="Model Accuracies with MARS")
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")
modelnames
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
modelLookup('earth')
# Set the seed for reproducibility
set.seed(100)
# Train the model using randomForest and predict on the training data itself.
model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)
plot(model_mars, main="Model Accuracies with MARS")
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")
modelnames
# Step 1: Impute missing values
testData2 <- predict(preProcess_missingdata_model, testData)
# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)
# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)
# View
head(testData4[, 1:10])
# Predict on testData
predicted <- predict(model_mars, testData4)
head(predicted)
# Compute the confusion matrix
confusionMatrix(reference = testData$Purchase, data = predicted, mode='everything', positive='MM')
# Define the training control
fitControl <- trainControl(
method = 'cv',                   # k-fold cross validation
number = 5,                      # number of folds
savePredictions = 'final',       # saves predictions for optimal tuning parameter
classProbs = T,                  # should class probabilities be returned
summaryFunction=twoClassSummary  # results summary function
)
# Step 1: Tune hyper parameters by setting tuneLength
set.seed(100)
model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl)
model_mars2
# Step 2: Predict on testData and Compute the confusion matrix
predicted2 <- predict(model_mars2, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted2, mode='everything', positive='MM')
model_mars2
# Step 1: Define the tuneGrid
marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10),
degree = c(1, 2, 3))
# Step 2: Tune hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)
model_mars3
# Step 3: Predict on testData and Compute the confusion matrix
predicted3 <- predict(model_mars3, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted3, mode='everything', positive='MM')
##  Training Adaboost
set.seed(100)
# Train the model using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
model_adaboost
##  Training Random Forest
set.seed(100)
# Train the model using rf
model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)
model_rf
##  Training xgBoost Dart
set.seed(100)
# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
##  Training xgBoost Dart
set.seed(100)
# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
library(caretEnsemble)
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE)
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList)
data <- read.csv('shelf-life-study-data-for-analytics-challenge_prediction.csv')
library(readr)
shelf_life_study_data_for_analytics_challenge_prediction <- read_csv("~/GitHub/datathon2019/pepsi.challenge/shelf-life-study-data-for-analytics-challenge_prediction.csv")
View(shelf_life_study_data_for_analytics_challenge_prediction)
library(readr)
shelf_life_study_data_for_analytics_challenge_prediction <- read_csv("~/GitHub/datathon2019/pepsi.challenge/shelf-life-study-data-for-analytics-challenge_prediction.csv")
View(shelf_life_study_data_for_analytics_challenge_prediction)
?read.csv
View(shelf_life_study_data_for_analytics_challenge_prediction)
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("~/GitHub/datathon2019/pepsi.challenge/shelf-life-study-data-for-analytics-challenge_prediction.csv", row)
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("shelf-life-study-data-for-analytics-challenge_prediction.csv")
setwd("~/GitHub/datathon2019/pepsi.challenge")
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("shelf-life-study-data-for-analytics-challenge_prediction.csv")
View(shelf_life_study_data_for_analytics_challenge_prediction)
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("pepsi.data.csv")
partitionedData <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
anyNA(trainData)
View(trainData)
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("pepsi.data.csv", na.strings = '')
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
View(trainData)
num <- 0
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)) num = num + 1
}
num
ncol(shelf_life_study_data_for_analytics_challenge_prediction)
num <- 0
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)) num = num + 1
}
num
badrows <- c(badrows, i)
num <- 0
badrows <- c()
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)) num = num + 1
badrows <- c(badrows, i)
}
data <- read.csv('shelf-life-study-data-for-analytics-challenge_prediction.csv')
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("pepsi.data.csv", na.strings = '')
num <- 0
badrows <- c()
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
num = num + 1
badrows <- c(badrows, i)
}
}
num
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[-badrows,]
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
badcols <- c()
for(i in 1:ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
cv<- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[,i]))
if(c > .5*nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
badcols <- c(badcols,i)
}
}
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
badcols <- c()
for(i in 1:ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
cv<- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[,i]))
if(c > .5*nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
badcols <- c(badcols,i)
}
}
badcols <- c()
for(i in 1:ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
c<- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[,i]))
if(c > .5*nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
badcols <- c(badcols,i)
}
}
shelf_life_study_data_for_analytics_challenge_prediction[,badcols]
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[,-badcols]
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  fill in missing data using impution
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
View(shelf_life_study_data_for_analytics_challenge_prediction)
View(trainData)
data <- read.csv('shelf-life-study-data-for-analytics-challenge_prediction.csv')
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("pepsi.data.csv", na.strings = '')
num <- 0
badrows <- c()
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
num = num + 1
badrows <- c(badrows, i)
}
}
num
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[-badrows,]
badcols <- c()
for(i in 1:ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
c<- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[,i]))
if(c > .5*nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
badcols <- c(badcols,i)
}
}
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[,-badcols]
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
View(trainData)
data <- read.csv('shelf-life-study-data-for-analytics-challenge_prediction.csv')
shelf_life_study_data_for_analytics_challenge_prediction <- read.csv("pepsi.data.csv", na.strings = '')
num <- 0
badrows <- c()
for(i in 1:nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
c <- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[i,]))
if(c > .5 * ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
num = num + 1
badrows <- c(badrows, i)
}
}
num
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[-badrows,]
badcols <- c()
for(i in 1:ncol(shelf_life_study_data_for_analytics_challenge_prediction)){
c<- sum(is.na(shelf_life_study_data_for_analytics_challenge_prediction[,i]))
if(c > .5*nrow(shelf_life_study_data_for_analytics_challenge_prediction)){
badcols <- c(badcols,i)
}
}
shelf_life_study_data_for_analytics_challenge_prediction <- shelf_life_study_data_for_analytics_challenge_prediction[,-badcols[1:3]]
trainRowNumbers <- createDataPartition(shelf_life_study_data_for_analytics_challenge_prediction$Difference.From.Fresh, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- shelf_life_study_data_for_analytics_challenge_prediction[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- shelf_life_study_data_for_analytics_challenge_prediction[-trainRowNumbers,]
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
View(trainData)
View(trainData)
##  converting categorical variables to numeric to be used in machine learning
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(DifferenceFromFresh ~ ., data=trainData)
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
